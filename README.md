# 🌧️ Rainfall Prediction using Machine Learning

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/Scikit_Learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-11557C?style=for-the-badge&logo=matplotlib&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)

## 📌 Overview

This project aims to predict rainfall using various machine learning algorithms. It analyzes historical weather data and applies classification techniques to forecast whether it will rain or not.

## 📊 Algorithms Performance

| Algorithm               | Accuracy | Precision (No/Yes) | Recall (No/Yes) | F1-score (No/Yes) |
|-------------------------|----------|--------------------|----------------|-----------------|
| 🌳 Decision Tree       | 64%      | 0.41 / 0.73      | 0.39 / 0.75    | 0.40 / 0.74     |
| 📈 Logistic Regression | 78.3%    | 0.77 / 0.79      | 0.43 / 0.94    | 0.56 / 0.86     |
| 🔍 SVM                 | 68.9%    | 0.00 / 0.69      | 0.00 / 1.00    | 0.00 / 0.82     |
| 🚀 Gradient Boosting   | 72.9%    | 0.60 / 0.76      | 0.39 / 0.88    | 0.47 / 0.82     |
| ⚡ AdaBoost              | 75.7%    | 0.63 / 0.80      | 0.52 / 0.86    | 0.57 / 0.83     |
| 📊 Naive Bayes         | 75.7%    | 0.63 / 0.80      | 0.52 / 0.86    | 0.57 / 0.83     |

## 🛠 Tech Stack



## 🚀 Installation & Usage

```bash
# Clone the repository
git clone https://github.com/your-username/Rainfall-prediction.git

# Navigate to the project directory
cd Rainfall-prediction

# Run the model
python Rainfall_Prediction.py
```

## 📌 Results & Insights

- **Logistic Regression performed the best**, achieving an accuracy of **78.3%**.
- **SVM had issues classifying 'No' cases**, as indicated by its poor recall for the 'No' class.
- **Ensemble methods (AdaBoost & Gradient Boosting)** improved performance significantly.
- **Naive Bayes performed well despite its simplicity**, matching AdaBoost's accuracy.


## 🤝 Contributing

Contributions are welcome! Feel free to fork this repository and submit pull requests.

## 📬 Contact

📧 Email: [mekalcheruvunithish2580@gmail.com](mailto\:mekalcheruvunithish2580@gmail.com)\
🔗 GitHub: [nithish-m123](https://github.com/nithish-m123)\
🚀 LinkedIn: [nithish-m123](https://linkedin.com/in/nithish-m123)

---


